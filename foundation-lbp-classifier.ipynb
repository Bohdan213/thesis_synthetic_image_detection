{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.602070Z",
     "iopub.status.busy": "2025-04-26T19:44:25.601581Z",
     "iopub.status.idle": "2025-04-26T19:44:25.607507Z",
     "shell.execute_reply": "2025-04-26T19:44:25.606623Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.602025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.v2 import JPEG, RandomApply,GaussianBlur\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import timm\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import classification_report, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.609055Z",
     "iopub.status.busy": "2025-04-26T19:44:25.608731Z",
     "iopub.status.idle": "2025-04-26T19:44:25.624014Z",
     "shell.execute_reply": "2025-04-26T19:44:25.623323Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.609033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_RANDOM_SEED = 2003\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seedBasic(DEFAULT_RANDOM_SEED)\n",
    "seedTorch(DEFAULT_RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up configurations for trainin and/or evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.625744Z",
     "iopub.status.busy": "2025-04-26T19:44:25.625540Z",
     "iopub.status.idle": "2025-04-26T19:44:25.639654Z",
     "shell.execute_reply": "2025-04-26T19:44:25.638990Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.625726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SYNCLR_PATH = \"\" # if path is valid - use SynClr, othervise - DinoV2\n",
    "\n",
    "PRETRAINED_CPT = \"foundation_plus_lbp_dinov2_sd14.pth\" # init weights will be used if pretrained_checkpoint is invalid\n",
    "TEST = True # Should model be tested? if pretrained_checkpoint is invalid and TRAIN is False - init weights will be used\n",
    "TRAIN = True # Should model be trained? Disable if you are interested only in evaluation\n",
    "\n",
    "# How much patches should be used in test-time ensembling\n",
    "SIMPLE_PATCHES_NUM = 1\n",
    "# Set the number of test images for each class, but if the actual number of images is less than TEST_SIZE, actual number will be used.\n",
    "TEST_SIZE = 6_000\n",
    "\n",
    "# Enable or disable training augmentations\n",
    "TRAIN_AUG = True\n",
    "# JPEG compression uniform distribution range\n",
    "JPEG_INTERVAL_TRAIN = (90, 100)\n",
    "# Gaussian blur uniform distribution range\n",
    "BLUR_INTERVAL_TRAIN = (0.1, 1)\n",
    "# Probability for applying each training augmentation\n",
    "AUG_PROB = 0.1\n",
    "\n",
    "# Use 100% JPEG compression for the test set or not\n",
    "TEST_AUG_JPEG = False\n",
    "# Use 100% Gaussian blur for the test set or not\n",
    "TEST_AUG_BLUR = False\n",
    "# JPEG compression quality for the test set; applied only if TEST_AUG_JPEG = True\n",
    "JPEG_LEVEL_TEST = 90\n",
    "# Gaussian blur sigma level for the test set; applied only if TEST_AUG_BLUR = True\n",
    "BLUR_LEVEL_TEST = 1\n",
    "# Whether precision-recall curves should be plotted and saved in the /curves folder\n",
    "VISUALISE_CURVES = True\n",
    "\n",
    "\n",
    "# training setup\n",
    "BATCH_SIZE = 64\n",
    "EPOCH_NUM = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Dataset directory, with all generators sets in GenImage format. Example:\n",
    "# - dataset\n",
    "#\n",
    "# -- imagenet-ai-0419-sdv4\n",
    "#\n",
    "# --- val\n",
    "# ---- ai\n",
    "# ---- nature\n",
    "# --- train\n",
    "# ---- ai\n",
    "# ---- nature\n",
    "#\n",
    "# -- imagen3\n",
    "# --- val\n",
    "# ---- ai\n",
    "# ---- nature\n",
    "DATASET_DIR = \"dataset\"\n",
    "\n",
    "# folder with training set\n",
    "TRAIN_DATASET = \"imagenet-ai-0419-sdv4\"\n",
    "\n",
    "# set up a dictionary where the keys are the folder names in DATASET_DIR, and the values are True or False (indicating whether the folder should be evaluated).\n",
    "TEST_DATASETS = {\n",
    "    \"FLUX1-dev\": True,\n",
    "    \"imagen3\": True,\n",
    "    \"SDXL1\": True,\n",
    "    \"PixArt-XL-2-1024-MS\": True,\n",
    "    \"imagenet-midjourney\": True,\n",
    "    \"imagenet-ai-0419-sdv4\": True,\n",
    "    \"imagenet-ai-0424-sdv5\": True,\n",
    "    \"imagenet-ai-0508-adm\": True,\n",
    "    \"imagenet-glide\": True,\n",
    "    \"imagenet-ai-0424-wukong\": True,\n",
    "    \"imagenet-ai-0419-vqdm\": True,\n",
    "    \"imagenet-ai-0419-biggan\": True,\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block with utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.640704Z",
     "iopub.status.busy": "2025-04-26T19:44:25.640477Z",
     "iopub.status.idle": "2025-04-26T19:44:25.658343Z",
     "shell.execute_reply": "2025-04-26T19:44:25.657703Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.640686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_lbp(image):\n",
    "    \"\"\"\n",
    "    Compute the Local Binary Pattern (LBP) for an input grayscale image.\n",
    "    \"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    h, w = image.shape\n",
    "    lbp_image = np.zeros((h, w), dtype=np.uint8)\n",
    "\n",
    "    offsets = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "    \n",
    "    center = image[1:-1, 1:-1]\n",
    "    binary_pattern = np.zeros_like(center, dtype=np.uint8)\n",
    "\n",
    "    for i, (dy, dx) in enumerate(offsets):\n",
    "        neighbor = image[1+dy:h-1+dy, 1+dx:w-1+dx]\n",
    "        binary_pattern |= ((center > neighbor).astype(np.uint8) << i)\n",
    "\n",
    "    lbp_image[1:-1, 1:-1] = binary_pattern\n",
    "\n",
    "    return lbp_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.659336Z",
     "iopub.status.busy": "2025-04-26T19:44:25.659130Z",
     "iopub.status.idle": "2025-04-26T19:44:25.671973Z",
     "shell.execute_reply": "2025-04-26T19:44:25.671338Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.659309Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_patch_texture(patch):\n",
    "    \"\"\"\n",
    "    Compute path's texture diversity\n",
    "    \"\"\"\n",
    "    weight, height = patch.size\n",
    "    res = 0\n",
    "    patch = np.array(patch).astype(np.int64)\n",
    "    diff_horizontal = np.sum(np.abs(patch[:, :-1, :] - patch[:, 1:, :]))\n",
    "    diff_vertical = np.sum(np.abs(patch[:-1, :, :] - patch[1:, :, :]))\n",
    "    diff_diagonal = np.sum(np.abs(patch[:-1, :-1, :] - patch[1:, 1:, :]))\n",
    "    diff_diagonal += np.sum(np.abs(patch[1:, :-1, :] - patch[:-1, 1:, :]))\n",
    "    res = diff_horizontal + diff_vertical + diff_diagonal\n",
    "    return res.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.673011Z",
     "iopub.status.busy": "2025-04-26T19:44:25.672717Z",
     "iopub.status.idle": "2025-04-26T19:44:25.689483Z",
     "shell.execute_reply": "2025-04-26T19:44:25.688875Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.672984Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_patches(image, patch_size):\n",
    "    \"\"\"\n",
    "    Extract top SIMPLE_PATCHES_NUM patches with poor texture diversity\n",
    "    \"\"\"\n",
    "    resize_image = transforms.Resize((256, 256))\n",
    "    if min(image.size) < patch_size:\n",
    "        image = resize_image(image)\n",
    "\n",
    "    patches = []\n",
    "    image_width, image_height = image.size\n",
    "    num_patches = (image_width // patch_size) * (image_height // patch_size)\n",
    "    for i in range(num_patches):\n",
    "        top = random.randint(0, image_height - patch_size)\n",
    "        left = random.randint(0, image_width - patch_size)\n",
    "        \n",
    "        patches.append(transforms.functional.crop(image, top, left, patch_size, patch_size))\n",
    "    \n",
    "    patches.sort(key=lambda x: compute_patch_texture(x), reverse=False)\n",
    "\n",
    "    if len(patches) < SIMPLE_PATCHES_NUM and patches:\n",
    "        patches += [patches[0]] * (SIMPLE_PATCHES_NUM - len(patches))\n",
    "\n",
    "    return patches[:SIMPLE_PATCHES_NUM]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.691025Z",
     "iopub.status.busy": "2025-04-26T19:44:25.690806Z",
     "iopub.status.idle": "2025-04-26T19:44:25.711500Z",
     "shell.execute_reply": "2025-04-26T19:44:25.710858Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.691007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for training set\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        self.paths = []\n",
    "        self.model_name = model_name\n",
    "        self.get_paths()\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.RandomCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.transform_patch = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "        \n",
    "        self.resize = transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        self.jpeg_augmentation = JPEG(quality=JPEG_INTERVAL_TRAIN)\n",
    "        self.blur = GaussianBlur(kernel_size=5, sigma=BLUR_INTERVAL_TRAIN)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, image_label = self.paths[index]\n",
    "        \n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        if TRAIN_AUG:\n",
    "            if random.random() < AUG_PROB:\n",
    "                image = self.jpeg_augmentation(image)\n",
    "            if random.random() < AUG_PROB:\n",
    "                image = self.blur(image)\n",
    "        \n",
    "        patches = get_patches(image, 32)\n",
    "        processed_patches = []\n",
    "        \n",
    "        for patch in patches:\n",
    "            patch = patch.convert('L')\n",
    "            patch = compute_lbp(patch)\n",
    "            patch = Image.fromarray(patch)\n",
    "            patch = self.resize(patch)\n",
    "        \n",
    "            if self.transform_patch:\n",
    "                patch = self.transform_patch(patch)\n",
    "        \n",
    "            processed_patches.append(patch)\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        return (image, torch.stack(processed_patches)), image_label\n",
    "\n",
    "    def get_paths(self):\n",
    "        \"\"\"\n",
    "        Extract paths from the generator's training set\n",
    "        \"\"\"\n",
    "        ai_paths_train = os.listdir(os.path.join(DATASET_DIR, self.model_name, \"train/ai\"))\n",
    "        ai_paths_train = [(os.path.join(DATASET_DIR, self.model_name, \"train/ai\", path), 1) for path in ai_paths_train]\n",
    "        \n",
    "        real_paths_train = os.listdir(os.path.join(DATASET_DIR, self.model_name, \"train/nature\"))\n",
    "        real_paths_train = [(os.path.join(DATASET_DIR, self.model_name, \"train/nature\", path), 0) for path in real_paths_train]\n",
    "\n",
    "        self.paths = ai_paths_train + real_paths_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.713287Z",
     "iopub.status.busy": "2025-04-26T19:44:25.713100Z",
     "iopub.status.idle": "2025-04-26T19:44:25.732234Z",
     "shell.execute_reply": "2025-04-26T19:44:25.731574Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.713271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for test set\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name):\n",
    "        self.paths = []\n",
    "        self.model_name = model_name\n",
    "        self.get_paths()\n",
    "        \n",
    "        self.image_transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.transform_patch = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "        ])\n",
    "        \n",
    "        self.resize = transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        self.jpeg_augmentation = JPEG(quality=JPEG_LEVEL_TEST)\n",
    "        self.blur = GaussianBlur(kernel_size=5, sigma=BLUR_LEVEL_TEST)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path, image_label = self.paths[index]\n",
    "        \n",
    "        image = Image.open(path).convert('RGB')\n",
    "\n",
    "        if TEST_AUG_JPEG:\n",
    "            image = self.jpeg_augmentation(image)\n",
    "        if TEST_AUG_BLUR:\n",
    "            image = self.blur(image)\n",
    "        \n",
    "        patches = get_patches(image, 32)\n",
    "        processed_patches = []\n",
    "        \n",
    "        for patch in patches:\n",
    "            patch = patch.convert('L')\n",
    "            patch = compute_lbp(patch)\n",
    "            patch = Image.fromarray(patch)\n",
    "            patch = self.resize(patch)\n",
    "        \n",
    "            if self.transform_patch:\n",
    "                patch = self.transform_patch(patch)\n",
    "        \n",
    "            processed_patches.append(patch)\n",
    "\n",
    "        if self.image_transform:\n",
    "            image = self.image_transform(image)\n",
    "        \n",
    "        return (image, torch.stack(processed_patches)), image_label\n",
    "\n",
    "    def get_paths(self):\n",
    "        \"\"\"\n",
    "        Extract paths from the generator's test set\n",
    "        \"\"\"\n",
    "        ai_paths_test = os.listdir(os.path.join(DATASET_DIR, self.model_name, \"val/ai\"))\n",
    "        ai_paths_test = [(os.path.join(DATASET_DIR, self.model_name, \"val/ai\", path), 1) for path in ai_paths_test]\n",
    "        \n",
    "        real_paths_test = os.listdir(os.path.join(DATASET_DIR, self.model_name, \"val/nature\"))\n",
    "        real_paths_test = [(os.path.join(DATASET_DIR, self.model_name, \"val/nature\", path), 0) for path in real_paths_test]\n",
    "\n",
    "        self.paths = ai_paths_test[:TEST_SIZE] + real_paths_test[:TEST_SIZE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create datasets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:25.733687Z",
     "iopub.status.busy": "2025-04-26T19:44:25.733395Z",
     "iopub.status.idle": "2025-04-26T19:44:28.172808Z",
     "shell.execute_reply": "2025-04-26T19:44:28.172086Z",
     "shell.execute_reply.started": "2025-04-26T19:44:25.733659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = TrainDataset(TRAIN_DATASET)\n",
    "test_datasets = []\n",
    "for key, value in TEST_DATASETS.items():\n",
    "    if value:\n",
    "        test_datasets.append((TestDataset(key), key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:28.174148Z",
     "iopub.status.busy": "2025-04-26T19:44:28.173670Z",
     "iopub.status.idle": "2025-04-26T19:44:28.216186Z",
     "shell.execute_reply": "2025-04-26T19:44:28.215360Z",
     "shell.execute_reply.started": "2025-04-26T19:44:28.174115Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "test_loaders = [(DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4), name) for test_dataset, name in test_datasets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:28.217301Z",
     "iopub.status.busy": "2025-04-26T19:44:28.217036Z",
     "iopub.status.idle": "2025-04-26T19:44:28.232884Z",
     "shell.execute_reply": "2025-04-26T19:44:28.232100Z",
     "shell.execute_reply.started": "2025-04-26T19:44:28.217271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_dinoV2():\n",
    "    \"\"\"\n",
    "    Load pre-trained DINOv2 model's weights based on vit-l14 architecture from torch.hub\n",
    "    \"\"\"\n",
    "    backbone_model_real = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vitl14\", pretrained=True)\n",
    "    \n",
    "    for param in backbone_model_real.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    backbone_model_real.to(DEVICE)\n",
    "    print(\"DinoV2 loaded\")\n",
    "\n",
    "    return backbone_model_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:28.234031Z",
     "iopub.status.busy": "2025-04-26T19:44:28.233787Z",
     "iopub.status.idle": "2025-04-26T19:44:28.248597Z",
     "shell.execute_reply": "2025-04-26T19:44:28.247859Z",
     "shell.execute_reply.started": "2025-04-26T19:44:28.234013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_synclr():\n",
    "    \"\"\"\n",
    "    Load pre-trained SynCLR model's weights based on vit-l14 architecture from local file\n",
    "    \"\"\"\n",
    "    backbone_model_syn = timm.create_model('vit_large_patch14_224', pretrained=False, num_classes=0)\n",
    "\n",
    "    checkpoint = torch.load(SYNCLR_PATH, map_location='cuda')\n",
    "    state_dict = checkpoint.get('model', checkpoint)\n",
    "    \n",
    "    def remove_module_prefix(state_dict):\n",
    "        return {k.replace(\"module.visual.\", \"\"): v for k, v in state_dict.items()}\n",
    "    \n",
    "    state_dict = remove_module_prefix(state_dict)\n",
    "    backbone_model_syn.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    for param in backbone_model_syn.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    backbone_model_syn.to(DEVICE)\n",
    "    \n",
    "    print(\"Synclr loaded\")\n",
    "\n",
    "    return backbone_model_syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load foundation model based on config setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:28.249547Z",
     "iopub.status.busy": "2025-04-26T19:44:28.249332Z",
     "iopub.status.idle": "2025-04-26T19:44:33.636613Z",
     "shell.execute_reply": "2025-04-26T19:44:33.635873Z",
     "shell.execute_reply.started": "2025-04-26T19:44:28.249529Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "use_synclr = True\n",
    "if os.path.isfile(SYNCLR_PATH):\n",
    "    try:\n",
    "        print(\"Use SynClr\")\n",
    "        backbone_foundation_model = load_synclr()\n",
    "    except Exception as e:\n",
    "        print(\"While loading SynClr something went wrong\")\n",
    "        print(\"Use DinoV2\")\n",
    "        backbone_foundation_model = load_dinoV2()\n",
    "        use_synclr = False\n",
    "else:\n",
    "    print(\"Use DinoV2\")\n",
    "    backbone_foundation_model = load_dinoV2()\n",
    "    use_synclr = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise ResNet-18 model without pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.639132Z",
     "iopub.status.busy": "2025-04-26T19:44:33.638876Z",
     "iopub.status.idle": "2025-04-26T19:44:33.808705Z",
     "shell.execute_reply": "2025-04-26T19:44:33.807839Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.639110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lbp_resnet = models.resnet18(pretrained=False)\n",
    "lbp_resnet.conv1 = nn.Conv2d(\n",
    "    in_channels=1,\n",
    "    out_channels=lbp_resnet.conv1.out_channels,\n",
    "    kernel_size=lbp_resnet.conv1.kernel_size,\n",
    "    stride=lbp_resnet.conv1.stride,\n",
    "    padding=lbp_resnet.conv1.padding,\n",
    "    bias=False\n",
    ")\n",
    "\n",
    "lbp_resnet.fc = nn.Identity()\n",
    "\n",
    "lbp_resnet.to(DEVICE)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define standart implementation for MLP head with GELU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.810238Z",
     "iopub.status.busy": "2025-04-26T19:44:33.809993Z",
     "iopub.status.idle": "2025-04-26T19:44:33.815134Z",
     "shell.execute_reply": "2025-04-26T19:44:33.814207Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.810218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(hidden_features, out_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define two-branch architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.816116Z",
     "iopub.status.busy": "2025-04-26T19:44:33.815896Z",
     "iopub.status.idle": "2025-04-26T19:44:33.831085Z",
     "shell.execute_reply": "2025-04-26T19:44:33.830371Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.816098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FoundationLbpClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, backbone_foundation_model, lbp_resnet):\n",
    "        super().__init__()\n",
    "        self.backbone_foundation_model = backbone_foundation_model\n",
    "        self.lbp_resnet = lbp_resnet\n",
    "        self.mlp = Mlp(1024, 512, 1)\n",
    "\n",
    "        self.proj_foundation = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, patches):\n",
    "        features_foundation = self.backbone_foundation_model(x)\n",
    "        \n",
    "        patch_features = [self.lbp_resnet(patch) for patch in patches]\n",
    "        patch_features = torch.stack(patch_features)\n",
    "        features_lbp = patch_features.mean(dim=1)\n",
    "        features_lbp = features_lbp.squeeze()\n",
    "        \n",
    "        features_foundation_projected = self.proj_foundation(features_foundation)\n",
    "\n",
    "        features_concat = torch.cat((features_lbp, features_foundation_projected), dim=-1)\n",
    "        \n",
    "        out = self.mlp(features_concat)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.848978Z",
     "iopub.status.busy": "2025-04-26T19:44:33.848718Z",
     "iopub.status.idle": "2025-04-26T19:44:33.864971Z",
     "shell.execute_reply": "2025-04-26T19:44:33.864174Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.848959Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save(model, name):\n",
    "    \"\"\"\n",
    "    Save weights, ignore foundation model's weights\n",
    "    \"\"\"\n",
    "    state_dict = model.state_dict()\n",
    "    \n",
    "    filtered_state_dict = {\n",
    "        k: v for k, v in state_dict.items()\n",
    "        if not k.startswith(\"backbone_foundation_model\")\n",
    "    }\n",
    "    \n",
    "    torch.save({\n",
    "        'backbone_foundation_model_classifier': filtered_state_dict\n",
    "    }, f\"gelu_neighbor_center_{name}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.865896Z",
     "iopub.status.busy": "2025-04-26T19:44:33.865664Z",
     "iopub.status.idle": "2025-04-26T19:44:33.876656Z",
     "shell.execute_reply": "2025-04-26T19:44:33.875845Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.865877Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(model, test_loaders, visualise_curves=True):\n",
    "    \"\"\"\n",
    "    Validate model on test sets, plot precision-recall curves if needed\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for test, set_name in test_loaders:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        true_labels = []\n",
    "        predicted_labels = []\n",
    "        probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (images, patches), labels in test:\n",
    "                images, patches, labels = images.to(DEVICE), patches.to(DEVICE), labels.float().to(DEVICE)\n",
    "                labels = labels.view(-1, 1)\n",
    "                outputs = model(images, patches)\n",
    "                probs = outputs\n",
    "                predicted = (probs >= 0.5).float()\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                true_labels.append(labels.cpu())\n",
    "                predicted_labels.append(predicted.cpu())\n",
    "                probabilities.append(probs.cpu())\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Validation on {set_name}: Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "        true_labels = torch.cat(true_labels).numpy()\n",
    "        predicted_labels = torch.cat(predicted_labels).numpy()\n",
    "        probabilities = torch.cat(probabilities).numpy()\n",
    "\n",
    "        print(classification_report(true_labels, predicted_labels))\n",
    "\n",
    "        precision, recall, thresholds = precision_recall_curve(true_labels, probabilities)\n",
    "        avg_precision = average_precision_score(true_labels, probabilities)\n",
    "        \n",
    "        if visualise_curves:\n",
    "            threshold_point = 0.5\n",
    "            preds_at_05 = (probabilities >= threshold_point).astype(float)\n",
    "            p_05 = precision_score(true_labels, preds_at_05)\n",
    "            r_05 = recall_score(true_labels, preds_at_05)\n",
    "            \n",
    "            plt.figure()\n",
    "            plt.plot(recall, precision, label=None)\n",
    "            \n",
    "            plt.axvline(x=r_05, color='red', linestyle='--')\n",
    "            \n",
    "            plt.scatter(r_05, p_05, color='red', zorder=5)\n",
    "            \n",
    "            plt.text(0.02, 0.02, f'AP = {avg_precision:.2f}', transform=plt.gca().transAxes,\n",
    "                     fontsize=18, verticalalignment='bottom', horizontalalignment='left',\n",
    "                     bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.7))\n",
    "            \n",
    "            plt.xlabel('Recall', fontsize=16)\n",
    "            plt.ylabel('Precision', fontsize=16)\n",
    "            plt.title(f'Precision-Recall curve ({set_name})', fontsize=18)\n",
    "            plt.xticks(fontsize=14)\n",
    "            plt.yticks(fontsize=14)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            os.makedirs('curves', exist_ok=True)\n",
    "            filename = f'curves/pr_curve_{set_name}.png'\n",
    "            plt.savefig(filename)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.877862Z",
     "iopub.status.busy": "2025-04-26T19:44:33.877551Z",
     "iopub.status.idle": "2025-04-26T19:44:33.895606Z",
     "shell.execute_reply": "2025-04-26T19:44:33.894769Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.877824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, scheduler):\n",
    "    \"\"\"\n",
    "    Training code\n",
    "    \"\"\"\n",
    "    for epoch in range(EPOCH_NUM):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "    \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCH_NUM}\", leave=True)\n",
    "        \n",
    "        for batch_idx, ((images, patches), labels) in enumerate(progress_bar):\n",
    "            images, patches, labels = images.to(DEVICE), patches.to(DEVICE), labels.to(DEVICE)\n",
    "            labels = labels.float().view(-1, 1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, patches)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            predictions = (outputs >= 0.5).float()\n",
    "    \n",
    "            correct_train += (predictions == labels).sum().item()\n",
    "            total_train += labels.size(0)\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{running_loss / (total_train // labels.size(0)):.4f}\", \n",
    "                                     acc=f\"{100 * correct_train / total_train:.2f}%\")\n",
    "    \n",
    "            scheduler.step()\n",
    "\n",
    "        model.eval()\n",
    "        save(model, f\"synclr_epoch_{epoch}\" if use_synclr else f\"dinov2_epoch_{epoch}\")\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{EPOCH_NUM}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-26T19:44:33.896696Z",
     "iopub.status.busy": "2025-04-26T19:44:33.896420Z",
     "iopub.status.idle": "2025-04-26T19:44:33.913077Z",
     "shell.execute_reply": "2025-04-26T19:44:33.912353Z",
     "shell.execute_reply.started": "2025-04-26T19:44:33.896668Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_cpt(model):\n",
    "    \"\"\"\n",
    "    Load pre-trained weights for proposed architecture\n",
    "    \"\"\"\n",
    "    if os.path.isfile(PRETRAINED_CPT):\n",
    "        try:\n",
    "            print(\"Use pretrained checkpoint\")\n",
    "            checkpoint = torch.load(PRETRAINED_CPT, map_location=DEVICE)\n",
    "            model.load_state_dict(checkpoint[\"backbone_foundation_model_classifier\"], strict=False)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"While loading pretrained checkpoint something went wrong\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run train and/or evaluation process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = FoundationLbpClassifier(backbone_foundation_model, lbp_resnet)\n",
    "model = load_cpt(model)\n",
    "model.to(DEVICE)\n",
    "\n",
    "if TRAIN:\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    params = [\n",
    "        {'params': model.proj_foundation.parameters(), 'lr': LEARNING_RATE / 2},\n",
    "        {'params': model.lbp_resnet.parameters(), 'lr': LEARNING_RATE},\n",
    "    ]\n",
    "    optimizer = optim.Adam(params, lr=LEARNING_RATE)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=len(train_dataset) // BATCH_SIZE // 2, gamma=0.5)\n",
    "    train(model, train_loader, optimizer, criterion, scheduler)\n",
    "\n",
    "if TEST:\n",
    "    model.eval()\n",
    "    validate(model, test_loaders, visualise_curves=VISUALISE_CURVES)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7082071,
     "sourceId": 11322853,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7082319,
     "sourceId": 11323182,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7082461,
     "sourceId": 11323380,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7082579,
     "sourceId": 11323537,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7082627,
     "sourceId": 11323607,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7082720,
     "sourceId": 11323725,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7084984,
     "sourceId": 11326673,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7085601,
     "sourceId": 11327465,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7138449,
     "sourceId": 11397920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7138495,
     "sourceId": 11397978,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7138676,
     "sourceId": 11398213,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7138866,
     "sourceId": 11398486,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7194182,
     "sourceId": 11478463,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7194225,
     "sourceId": 11478523,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 302736,
     "modelInstanceId": 281856,
     "sourceId": 336832,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 310208,
     "modelInstanceId": 289470,
     "sourceId": 346461,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 310210,
     "modelInstanceId": 289472,
     "sourceId": 346463,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 314008,
     "modelInstanceId": 293372,
     "sourceId": 351505,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
